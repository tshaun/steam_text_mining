{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "app_name\n",
      "Dota 2                            73541\n",
      "Rocket League                     54227\n",
      "The Witcher 3: Wild Hunt          31853\n",
      "DOOM                              20682\n",
      "Counter-Strike                    12353\n",
      "Call of Duty: Modern Warfare 3     5197\n",
      "NBA 2K16                           2534\n",
      "The Elder Scrolls V: Skyrim        2260\n",
      "Football Manager 2016              1742\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('preprocessed_reviews.csv')\n",
    "\n",
    "# Get the number of reviews for each product\n",
    "product_review_count = df['app_name'].value_counts()\n",
    "\n",
    "\n",
    "print(product_review_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>app_id</th>\n",
       "      <th>app_name</th>\n",
       "      <th>review_text</th>\n",
       "      <th>review_score</th>\n",
       "      <th>review_votes</th>\n",
       "      <th>tokens</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>cleaned_review_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>Counter-Strike</td>\n",
       "      <td>ruined my life</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>['ruined', 'life']</td>\n",
       "      <td>-0.4767</td>\n",
       "      <td>ruined life</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>Counter-Strike</td>\n",
       "      <td>this will be more of a my experience with this...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>['experience', 'game', 'type', 'review', 'sayi...</td>\n",
       "      <td>0.9961</td>\n",
       "      <td>experience game type review saying things like...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>Counter-Strike</td>\n",
       "      <td>this game saved my virginity</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>['game', 'saved', 'virginity']</td>\n",
       "      <td>0.4215</td>\n",
       "      <td>game saved virginity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>Counter-Strike</td>\n",
       "      <td>do you like original games  do you like games...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>['like', 'original', 'games', 'like', 'games',...</td>\n",
       "      <td>0.8817</td>\n",
       "      <td>like original games like games dont lag like g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td>Counter-Strike</td>\n",
       "      <td>easy to learn hard to master</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>['easy', 'learn', 'hard', 'master']</td>\n",
       "      <td>0.3612</td>\n",
       "      <td>easy learn hard master</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   app_id        app_name                                        review_text  \\\n",
       "0      10  Counter-Strike                                     ruined my life   \n",
       "1      10  Counter-Strike  this will be more of a my experience with this...   \n",
       "2      10  Counter-Strike                       this game saved my virginity   \n",
       "3      10  Counter-Strike   do you like original games  do you like games...   \n",
       "4      10  Counter-Strike             easy to learn hard to master             \n",
       "\n",
       "   review_score  review_votes  \\\n",
       "0             1             0   \n",
       "1             1             1   \n",
       "2             1             0   \n",
       "3             1             0   \n",
       "4             1             1   \n",
       "\n",
       "                                              tokens  sentiment  \\\n",
       "0                                 ['ruined', 'life']    -0.4767   \n",
       "1  ['experience', 'game', 'type', 'review', 'sayi...     0.9961   \n",
       "2                     ['game', 'saved', 'virginity']     0.4215   \n",
       "3  ['like', 'original', 'games', 'like', 'games',...     0.8817   \n",
       "4                ['easy', 'learn', 'hard', 'master']     0.3612   \n",
       "\n",
       "                                 cleaned_review_text  \n",
       "0                                        ruined life  \n",
       "1  experience game type review saying things like...  \n",
       "2                               game saved virginity  \n",
       "3  like original games like games dont lag like g...  \n",
       "4                             easy learn hard master  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" import string\\nfrom nltk.corpus import stopwords\\nfrom nltk.tokenize import word_tokenize\\nimport nltk\\n\\n# Download stopwords if not already downloaded\\nnltk.download('stopwords')\\nnltk.download('punkt')\\n\\n# Define a function to preprocess text\\ndef preprocess_text(text):\\n    # Convert to lowercase\\n    text = text.lower()\\n    # Remove punctuation\\n    text = text.translate(str.maketrans('', '', string.punctuation))\\n    # Tokenize text\\n    tokens = word_tokenize(text)\\n    # Remove stopwords\\n    tokens = [word for word in tokens if word not in stopwords.words('english')]\\n    return ' '.join(tokens)\\n\\n# Apply the function to the review_text column\\ndf['cleaned_review_text'] = df['review_text'].apply(lambda x: preprocess_text(x) if isinstance(x, str) else x)\\n\\nprint(df[['review_text', 'cleaned_review_text']].head()) \""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "\n",
    "# Download stopwords if not already downloaded\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Define a function to preprocess text\n",
    "def preprocess_text(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    # Tokenize text\n",
    "    tokens = word_tokenize(text)\n",
    "    # Remove stopwords\n",
    "    tokens = [word for word in tokens if word not in stopwords.words('english')]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Apply the function to the review_text column\n",
    "df['cleaned_review_text'] = df['review_text'].apply(lambda x: preprocess_text(x) if isinstance(x, str) else x)\n",
    "\n",
    "print(df[['review_text', 'cleaned_review_text']].head()) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 cleaned_review_text  review_score  sentiment  \\\n",
      "0                                        ruined life             1    -0.4767   \n",
      "1  experience game type review saying things like...             1     0.9961   \n",
      "2                               game saved virginity             1     0.4215   \n",
      "3  like original games like games dont lag like g...             1     0.8817   \n",
      "4                             easy learn hard master             1     0.3612   \n",
      "\n",
      "   sarcasm  \n",
      "0        1  \n",
      "1        0  \n",
      "2        0  \n",
      "3        0  \n",
      "4        0  \n"
     ]
    }
   ],
   "source": [
    "# Define a function to detect sarcasm\n",
    "def detect_sarcasm(row):\n",
    "    # Check for rating-text mismatch\n",
    "    if row['review_score'] == 1 and row['sentiment'] < 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# Apply the function to the DataFrame\n",
    "df['sarcasm'] = df.apply(detect_sarcasm, axis=1)\n",
    "\n",
    "print(df[['cleaned_review_text', 'review_score', 'sentiment', 'sarcasm']].head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sarcasm\n",
      "0    182283\n",
      "1     22106\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df['sarcasm'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                                          ruined life\n",
      "1    experience game type review saying things like...\n",
      "2                                 game saved virginity\n",
      "3    like original games like games dont lag like g...\n",
      "4                               easy learn hard master\n",
      "5                                      r revolver play\n",
      "6                        still better call duty ghosts\n",
      "7    cant buy skins cases keys stickers gaben cant ...\n",
      "8    counterstrike ok years unlimited fun friends f...\n",
      "9    every server spanish french fluently swear lan...\n",
      "Name: cleaned_review_text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df['cleaned_review_text'].head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "review_votes\n",
      "0    173422\n",
      "1     30967\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df['review_votes'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   aa  aaa  aaaa  aaaaa  aaaaaaaaaa  aaaaaaaaaaa  \\\n",
      "0   0    0     0      0           0            0   \n",
      "1   0    0     0      0           0            0   \n",
      "2   0    0     0      0           0            0   \n",
      "3   0    0     0      0           0            0   \n",
      "4   0    0     0      0           0            0   \n",
      "\n",
      "   aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa  \\\n",
      "0                                 0   \n",
      "1                                 0   \n",
      "2                                 0   \n",
      "3                                 0   \n",
      "4                                 0   \n",
      "\n",
      "   aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa  \\\n",
      "0                                                  0                \n",
      "1                                                  0                \n",
      "2                                                  0                \n",
      "3                                                  0                \n",
      "4                                                  0                \n",
      "\n",
      "   aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaahhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh  \\\n",
      "0                                                  0                            \n",
      "1                                                  0                            \n",
      "2                                                  0                            \n",
      "3                                                  0                            \n",
      "4                                                  0                            \n",
      "\n",
      "   aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaahhhh  ...  zyzz  zz  zzz  zzzz  \\\n",
      "0                                        0  ...     0   0    0     0   \n",
      "1                                        0  ...     0   0    0     0   \n",
      "2                                        0  ...     0   0    0     0   \n",
      "3                                        0  ...     0   0    0     0   \n",
      "4                                        0  ...     0   0    0     0   \n",
      "\n",
      "   zzzzquiet  zzzzz  zzzzzz  zzzzzzzzzzzz  zzzzzzzzzzzzzzzzzzz  \\\n",
      "0          0      0       0             0                    0   \n",
      "1          0      0       0             0                    0   \n",
      "2          0      0       0             0                    0   \n",
      "3          0      0       0             0                    0   \n",
      "4          0      0       0             0                    0   \n",
      "\n",
      "   zzzzzzzzzzzzzzzzzzzzzzzzzzz  \n",
      "0                            0  \n",
      "1                            0  \n",
      "2                            0  \n",
      "3                            0  \n",
      "4                            0  \n",
      "\n",
      "[5 rows x 93228 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "import scipy.sparse\n",
    "\n",
    "# Initialize the TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "# Fit and transform the cleaned_review_text column\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(df['cleaned_review_text'].dropna())\n",
    "\n",
    "# Convert the TF-IDF matrix to a DataFrame (using sparse matrix format)\n",
    "tfidf_df = pd.DataFrame.sparse.from_spmatrix(tfidf_matrix, columns=tfidf_vectorizer.get_feature_names_out())\n",
    "\n",
    "print(tfidf_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-zero entries: 3714730\n",
      "Total elements: 18501935652\n"
     ]
    }
   ],
   "source": [
    "nonzero_count = tfidf_matrix.nnz\n",
    "total_elements = tfidf_matrix.shape[0] * tfidf_matrix.shape[1]\n",
    "print(\"Non-zero entries:\", nonzero_count)\n",
    "print(\"Total elements:\", total_elements)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "life      0.570045\n",
      "ruined    0.821614\n",
      "Name: 0, dtype: Sparse[float64, 0]\n"
     ]
    }
   ],
   "source": [
    "# Print the nonzero tokens for the first review\n",
    "nonzero_tokens = tfidf_df.iloc[0][tfidf_df.iloc[0] != 0]\n",
    "print(nonzero_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nonzero counts for each row: [  2 140   3 ...   3 124  22]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>app_id</th>\n",
       "      <th>app_name</th>\n",
       "      <th>review_text</th>\n",
       "      <th>review_score</th>\n",
       "      <th>review_votes</th>\n",
       "      <th>tokens</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>cleaned_review_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>Counter-Strike</td>\n",
       "      <td>ruined my life</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>['ruined', 'life']</td>\n",
       "      <td>-0.4767</td>\n",
       "      <td>ruined life</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>Counter-Strike</td>\n",
       "      <td>this will be more of a my experience with this...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>['experience', 'game', 'type', 'review', 'sayi...</td>\n",
       "      <td>0.9961</td>\n",
       "      <td>experience game type review saying things like...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>Counter-Strike</td>\n",
       "      <td>this game saved my virginity</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>['game', 'saved', 'virginity']</td>\n",
       "      <td>0.4215</td>\n",
       "      <td>game saved virginity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>Counter-Strike</td>\n",
       "      <td>do you like original games  do you like games...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>['like', 'original', 'games', 'like', 'games',...</td>\n",
       "      <td>0.8817</td>\n",
       "      <td>like original games like games dont lag like g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td>Counter-Strike</td>\n",
       "      <td>easy to learn hard to master</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>['easy', 'learn', 'hard', 'master']</td>\n",
       "      <td>0.3612</td>\n",
       "      <td>easy learn hard master</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   app_id        app_name                                        review_text  \\\n",
       "0      10  Counter-Strike                                     ruined my life   \n",
       "1      10  Counter-Strike  this will be more of a my experience with this...   \n",
       "2      10  Counter-Strike                       this game saved my virginity   \n",
       "3      10  Counter-Strike   do you like original games  do you like games...   \n",
       "4      10  Counter-Strike             easy to learn hard to master             \n",
       "\n",
       "   review_score  review_votes  \\\n",
       "0             1             0   \n",
       "1             1             1   \n",
       "2             1             0   \n",
       "3             1             0   \n",
       "4             1             1   \n",
       "\n",
       "                                              tokens  sentiment  \\\n",
       "0                                 ['ruined', 'life']    -0.4767   \n",
       "1  ['experience', 'game', 'type', 'review', 'sayi...     0.9961   \n",
       "2                     ['game', 'saved', 'virginity']     0.4215   \n",
       "3  ['like', 'original', 'games', 'like', 'games',...     0.8817   \n",
       "4                ['easy', 'learn', 'hard', 'master']     0.3612   \n",
       "\n",
       "                                 cleaned_review_text  \n",
       "0                                        ruined life  \n",
       "1  experience game type review saying things like...  \n",
       "2                               game saved virginity  \n",
       "3  like original games like games dont lag like g...  \n",
       "4                             easy learn hard master  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nonzero_per_row = tfidf_matrix.getnnz(axis=1)\n",
    "print(\"Nonzero counts for each row:\", nonzero_per_row)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_16372\\2618862361.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_clean.drop(columns=['review_text', 'review_votes', 'sentiment'], inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# --- Data Preparation ---\n",
    "# Filter the DataFrame to include only rows with a cleaned review text.\n",
    "# This ensures that the TF-IDF features align with your target variable.\n",
    "df_clean = df.dropna(subset=['cleaned_review_text'])\n",
    "df_clean.drop(columns=['review_text', 'review_votes', 'sentiment'], inplace=True)\n",
    "\n",
    "# IMPORTANT:\n",
    "# The tfidf_matrix should have been computed on df['cleaned_review_text'].dropna()\n",
    "# so its rows correspond to df_clean.\n",
    "# If that’s not the case, re-run the vectorization on df_clean, e.g.:\n",
    "# tfidf_matrix = tfidf_vectorizer.fit_transform(df_clean['cleaned_review_text'])\n",
    "\n",
    "# Define features and target\n",
    "X = tfidf_matrix          # TF-IDF features (sparse matrix)\n",
    "y = df_clean['review_score']  # Target variable: 1 (positive/recommend) or -1 (negative/not recommend)\n",
    "\n",
    "# --- Train-Test Split ---\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# --- Oversampling using SMOTE ---\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Initialize SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "\n",
    "# Apply SMOTE to the training data\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8558147737579361\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.41      0.84      0.55      4161\n",
      "           1       0.98      0.86      0.91     35531\n",
      "\n",
      "    accuracy                           0.86     39692\n",
      "   macro avg       0.69      0.85      0.73     39692\n",
      "weighted avg       0.92      0.86      0.88     39692\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 3506   655]\n",
      " [ 5068 30463]]\n"
     ]
    }
   ],
   "source": [
    "# --- Model Training ---\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Initialize and train the Logistic Regression classifier\n",
    "lr = LogisticRegression(max_iter=1000, class_weight='balanced')\n",
    "lr.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# --- Model Evaluation ---\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = lr.predict(X_test)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of tfidf_matrix: (198459, 93228)\n",
      "Number of rows in df_clean: 198459\n",
      "tfidf_matrix was computed on df_clean['cleaned_review_text']\n"
     ]
    }
   ],
   "source": [
    "# Check the shape of tfidf_matrix\n",
    "print(\"Shape of tfidf_matrix:\", tfidf_matrix.shape)\n",
    "\n",
    "# Check the number of rows in df_clean\n",
    "print(\"Number of rows in df_clean:\", df_clean.shape[0])\n",
    "\n",
    "# Verify if they match\n",
    "if tfidf_matrix.shape[0] == df_clean.shape[0]:\n",
    "    print(\"tfidf_matrix was computed on df_clean['cleaned_review_text']\")\n",
    "else:\n",
    "    print(\"tfidf_matrix was NOT computed on df_clean['cleaned_review_text']\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# Initialize and train the SVM classifier\n",
    "svm = SVC(kernel='linear', class_weight='balanced', random_state=42)\n",
    "svm.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred_svm = svm.predict(X_test)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_svm))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_svm))\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_svm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['svm_model.pkl']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Export the SVM model to a file\n",
    "joblib.dump(svm, 'svm_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9312707850448453\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.67      0.68      0.67      4161\n",
      "           1       0.96      0.96      0.96     35531\n",
      "\n",
      "    accuracy                           0.93     39692\n",
      "   macro avg       0.82      0.82      0.82     39692\n",
      "weighted avg       0.93      0.93      0.93     39692\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 2810  1351]\n",
      " [ 1377 34154]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Initialize and train the Random Forest classifier\n",
    "rf = RandomForestClassifier(n_estimators=100, class_weight='balanced', random_state=42)\n",
    "rf.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_rf))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_rf))\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7565504383754913\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.27      0.77      0.40      4161\n",
      "           1       0.97      0.75      0.85     35531\n",
      "\n",
      "    accuracy                           0.76     39692\n",
      "   macro avg       0.62      0.76      0.62     39692\n",
      "weighted avg       0.89      0.76      0.80     39692\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 3220   941]\n",
      " [ 8722 26809]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# Initialize and train the Gradient Boosting classifier\n",
    "gb = GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
    "gb.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred_gb = gb.predict(X_test)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_gb))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_gb))\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_gb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gradient_boosting_model.pkl']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Export the Random Forest model to a file\n",
    "joblib.dump(rf, 'random_forest_model.pkl')\n",
    "\n",
    "# Export the Gradient Boosting model to a file\n",
    "joblib.dump(gb, 'gradient_boosting_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>app_id</th>\n",
       "      <th>app_name</th>\n",
       "      <th>review_score</th>\n",
       "      <th>tokens</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>cleaned_review_text</th>\n",
       "      <th>sarcasm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>Counter-Strike</td>\n",
       "      <td>1</td>\n",
       "      <td>['experience', 'game', 'type', 'review', 'sayi...</td>\n",
       "      <td>0.9961</td>\n",
       "      <td>experience game type review saying things like...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>Counter-Strike</td>\n",
       "      <td>1</td>\n",
       "      <td>['game', 'saved', 'virginity']</td>\n",
       "      <td>0.4215</td>\n",
       "      <td>game saved virginity</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>Counter-Strike</td>\n",
       "      <td>1</td>\n",
       "      <td>['like', 'original', 'games', 'like', 'games',...</td>\n",
       "      <td>0.8817</td>\n",
       "      <td>like original games like games dont lag like g...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td>Counter-Strike</td>\n",
       "      <td>1</td>\n",
       "      <td>['easy', 'learn', 'hard', 'master']</td>\n",
       "      <td>0.3612</td>\n",
       "      <td>easy learn hard master</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10</td>\n",
       "      <td>Counter-Strike</td>\n",
       "      <td>1</td>\n",
       "      <td>['r', 'revolver', 'play']</td>\n",
       "      <td>0.0516</td>\n",
       "      <td>r revolver play</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   app_id        app_name  review_score  \\\n",
       "1      10  Counter-Strike             1   \n",
       "2      10  Counter-Strike             1   \n",
       "3      10  Counter-Strike             1   \n",
       "4      10  Counter-Strike             1   \n",
       "5      10  Counter-Strike             1   \n",
       "\n",
       "                                              tokens  sentiment  \\\n",
       "1  ['experience', 'game', 'type', 'review', 'sayi...     0.9961   \n",
       "2                     ['game', 'saved', 'virginity']     0.4215   \n",
       "3  ['like', 'original', 'games', 'like', 'games',...     0.8817   \n",
       "4                ['easy', 'learn', 'hard', 'master']     0.3612   \n",
       "5                          ['r', 'revolver', 'play']     0.0516   \n",
       "\n",
       "                                 cleaned_review_text  sarcasm  \n",
       "1  experience game type review saying things like...        0  \n",
       "2                               game saved virginity        0  \n",
       "3  like original games like games dont lag like g...        0  \n",
       "4                             easy learn hard master        0  \n",
       "5                                    r revolver play        0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_16372\\2893596847.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_clean['review_score'] = df_clean['review_score'].apply(lambda x: 1 if x == 1 else 0)\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "\n",
    "# Load BERT tokenizer and model\n",
    "#tokenizer = BertTokenizer.from_pretrained(\"./saved_model\")\n",
    "#model = BertForSequenceClassification.from_pretrained(\"./saved_model\")\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Tokenization function\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"cleaned_review_text\"], padding=\"max_length\", truncation=True, max_length=128)\n",
    "\n",
    "df_clean['review_score'] = df_clean['review_score'].apply(lambda x: 1 if x == 1 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 198459/198459 [01:18<00:00, 2523.54 examples/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())  # Should return True if GPU is available\n",
    "\n",
    "# Convert to Hugging Face dataset\n",
    "dataset = Dataset.from_pandas(df_clean)\n",
    "dataset = dataset.map(tokenize_function, batched=True)\n",
    "dataset = dataset.remove_columns([\"tokens\"])\n",
    "dataset = dataset.rename_column(\"review_score\", \"labels\")\n",
    "dataset.set_format(\"torch\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "# Split dataset\n",
    "train_test_split = dataset.train_test_split(test_size=0.2)\n",
    "train_dataset = train_test_split[\"train\"]\n",
    "test_dataset = train_test_split[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['app_id', 'app_name', 'labels', 'cleaned_review_text', '__index_level_0__', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "    num_rows: 198459\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_16372\\464535618.py:19: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='29769' max='29769' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [29769/29769 49:52, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.288700</td>\n",
       "      <td>0.340627</td>\n",
       "      <td>0.894966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.339300</td>\n",
       "      <td>0.334525</td>\n",
       "      <td>0.894966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.263500</td>\n",
       "      <td>0.287105</td>\n",
       "      <td>0.912174</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2481' max='2481' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2481/2481 00:49]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results: {'eval_loss': 0.28710535168647766, 'eval_accuracy': 0.912173737780913, 'eval_runtime': 49.5607, 'eval_samples_per_second': 800.877, 'eval_steps_per_second': 50.06, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred  # Unpack predictions and actual labels\n",
    "    predictions = np.argmax(logits, axis=-1)  # Convert logits to class labels (0 or 1)\n",
    "    return {\"accuracy\": accuracy_score(labels, predictions)}  # Calculate accuracy\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    fp16=True if torch.cuda.is_available() else False\n",
    ")\n",
    "\n",
    "# Define trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# Train model\n",
    "trainer.train() \n",
    "\n",
    "eval_results = trainer.evaluate()\n",
    "print(f\"Evaluation results: {eval_results}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python312\\Lib\\site-packages\\transformers\\training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_2012\\3187048647.py:19: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "c:\\Python312\\Lib\\site-packages\\accelerate\\accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4409' max='4409' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4409/4409 00:58]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results: {'eval_loss': 0.33828869462013245, 'eval_model_preparation_time': 0.002, 'eval_accuracy': 0.9122533454298026, 'eval_runtime': 58.3698, 'eval_samples_per_second': 604.286, 'eval_steps_per_second': 75.536}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "# Compute accuracy\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)  # Get predicted class (0 or 1)\n",
    "    return {\"accuracy\": accuracy_score(labels, predictions)}\n",
    "\n",
    "# Ensure Trainer uses compute_metrics\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics  # Include this\n",
    ")\n",
    "\n",
    "# Evaluate the model\n",
    "eval_results = trainer.evaluate()\n",
    "print(f\"Evaluation results: {eval_results}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.33828869462013245\n"
     ]
    }
   ],
   "source": [
    "print(eval_results['eval_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidParameterError",
     "evalue": "The 'y_pred' parameter of accuracy_score must be an array-like or a sparse matrix. Got 0 instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidParameterError\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Compute accuracy\u001b[39;00m\n\u001b[0;32m      2\u001b[0m predictions \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(eval_results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meval_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Convert logits to class labels\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m \u001b[43maccuracy_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_dataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlabels\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpredictions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEvaluation results: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00meval_results\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccuracy\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:203\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    200\u001b[0m to_ignore \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mself\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcls\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    201\u001b[0m params \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m params\u001b[38;5;241m.\u001b[39marguments\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m to_ignore}\n\u001b[1;32m--> 203\u001b[0m \u001b[43mvalidate_parameter_constraints\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    204\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparameter_constraints\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaller_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__qualname__\u001b[39;49m\n\u001b[0;32m    205\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    211\u001b[0m         )\n\u001b[0;32m    212\u001b[0m     ):\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:95\u001b[0m, in \u001b[0;36mvalidate_parameter_constraints\u001b[1;34m(parameter_constraints, params, caller_name)\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     90\u001b[0m     constraints_str \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     91\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([\u001b[38;5;28mstr\u001b[39m(c)\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39mconstraints[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m or\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     92\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconstraints[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     93\u001b[0m     )\n\u001b[1;32m---> 95\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m InvalidParameterError(\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam_name\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m parameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcaller_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     97\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconstraints_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam_val\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     98\u001b[0m )\n",
      "\u001b[1;31mInvalidParameterError\u001b[0m: The 'y_pred' parameter of accuracy_score must be an array-like or a sparse matrix. Got 0 instead."
     ]
    }
   ],
   "source": [
    "# Compute accuracy\n",
    "predictions = np.argmax(eval_results[\"eval_loss\"], axis=-1)  # Convert logits to class labels\n",
    "\n",
    "\n",
    "accuracy = accuracy_score(test_dataset[\"labels\"], predictions)\n",
    "\n",
    "print(f\"Evaluation results: {eval_results}\")\n",
    "print(f\"Model Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved successfully.\n"
     ]
    }
   ],
   "source": [
    "# Save model and tokenizer\n",
    "model.save_pretrained(\"./saved_model\")\n",
    "tokenizer.save_pretrained(\"./saved_model\")\n",
    "print(\"Model saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.0+cpu\n",
      "0.34.2\n",
      "4.49.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import accelerate\n",
    "import transformers\n",
    "print(torch.__version__)\n",
    "print(accelerate.__version__)\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_16372\\4036765384.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_clean['predicted_sentiment'] = df_clean['cleaned_review_text'].apply(predict_sentiment)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>app_id</th>\n",
       "      <th>app_name</th>\n",
       "      <th>review_score</th>\n",
       "      <th>tokens</th>\n",
       "      <th>cleaned_review_text</th>\n",
       "      <th>predicted_sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>Counter-Strike</td>\n",
       "      <td>1</td>\n",
       "      <td>['ruined', 'life']</td>\n",
       "      <td>ruined life</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>Counter-Strike</td>\n",
       "      <td>1</td>\n",
       "      <td>['experience', 'game', 'type', 'review', 'sayi...</td>\n",
       "      <td>experience game type review saying things like...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>Counter-Strike</td>\n",
       "      <td>1</td>\n",
       "      <td>['game', 'saved', 'virginity']</td>\n",
       "      <td>game saved virginity</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>Counter-Strike</td>\n",
       "      <td>1</td>\n",
       "      <td>['like', 'original', 'games', 'like', 'games',...</td>\n",
       "      <td>like original games like games dont lag like g...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td>Counter-Strike</td>\n",
       "      <td>1</td>\n",
       "      <td>['easy', 'learn', 'hard', 'master']</td>\n",
       "      <td>easy learn hard master</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   app_id        app_name  review_score  \\\n",
       "0      10  Counter-Strike             1   \n",
       "1      10  Counter-Strike             1   \n",
       "2      10  Counter-Strike             1   \n",
       "3      10  Counter-Strike             1   \n",
       "4      10  Counter-Strike             1   \n",
       "\n",
       "                                              tokens  \\\n",
       "0                                 ['ruined', 'life']   \n",
       "1  ['experience', 'game', 'type', 'review', 'sayi...   \n",
       "2                     ['game', 'saved', 'virginity']   \n",
       "3  ['like', 'original', 'games', 'like', 'games',...   \n",
       "4                ['easy', 'learn', 'hard', 'master']   \n",
       "\n",
       "                                 cleaned_review_text  predicted_sentiment  \n",
       "0                                        ruined life                    1  \n",
       "1  experience game type review saying things like...                    1  \n",
       "2                               game saved virginity                    1  \n",
       "3  like original games like games dont lag like g...                    1  \n",
       "4                             easy learn hard master                    1  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Function to predict sentiment\n",
    "def predict_sentiment(text):\n",
    "    inputs = tokenizer(text, padding=\"max_length\", truncation=True, max_length=128, return_tensors=\"pt\")\n",
    "    \n",
    "    # Move inputs to the same device as the model\n",
    "    inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    logits = outputs.logits\n",
    "    prediction = torch.argmax(logits, dim=-1).item()\n",
    "    return prediction\n",
    "\n",
    "# Apply the trained model to predict sentiment for all reviews\n",
    "df_clean['predicted_sentiment'] = df_clean['cleaned_review_text'].apply(predict_sentiment)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean[df_clean['predicted_sentiment'] != df_clean['review_score']].head()\n",
    "\n",
    "df_clean.to_csv('predicted_sentiment.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
