{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   app_id        app_name                                        review_text  \\\n",
      "0      10  Counter-Strike                                    Ruined my life.   \n",
      "1      10  Counter-Strike  This will be more of a ''my experience with th...   \n",
      "2      10  Counter-Strike                      This game saved my virginity.   \n",
      "3      10  Counter-Strike  • Do you like original games? • Do you like ga...   \n",
      "4      10  Counter-Strike           Easy to learn, hard to master.             \n",
      "\n",
      "   review_score  review_votes  \n",
      "0             1             0  \n",
      "1             1             1  \n",
      "2             1             0  \n",
      "3             1             0  \n",
      "4             1             1  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"steam_reviews.csv\")\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#games = [\"Dota 2\", \"Skyrim\", \"Witcher 3\", \"Counter-Strike: Global Offensive\", \"Call of Duty\", \"Doom\", \"NBA\", \"Rocket League\", \"Football Manager\"]\n",
    "games = [\n",
    "    #RPG\n",
    "    \"Dota 2\",\n",
    "    \"The Elder Scrolls V: Skyrim\",\n",
    "    \"The Witcher 3: Wild Hunt\",\n",
    "\n",
    "    #FPS\n",
    "    \"Call of Duty: Modern Warfare 3\",\n",
    "    \"Counter-Strike\",\n",
    "    \"DOOM\",\n",
    "\n",
    "    #Sports\n",
    "    \"NBA 2K16\",\n",
    "    \"Rocket League\",\n",
    "    \"Football Manager 2016\"\n",
    "    ]\n",
    "\n",
    "df_filtered = df[df['app_name'].isin(games)]\n",
    "\n",
    "print(df_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nltk gensim pyLDAvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lda model to get the frequent words per topic\n",
    "import nltk\n",
    "import re\n",
    "import pandas as pd\n",
    "import gensim\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "from gensim import corpora\n",
    "from gensim.models import LdaModel, CoherenceModel\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "# Custom stopword list based on frequency graph\n",
    "stop_list = nltk.corpus.stopwords.words('english')\n",
    "stop_list += [\n",
    "    \"game\", \"play\", \"like\", \"good\", \"best\", \"one\", \"great\", \"really\", \"get\", \"time\",\n",
    "    \"ever\", \"played\", \"playing\", \"would\", \"even\", \"much\", \"hours\", \"love\", \"still\",\n",
    "    \"buy\", \"amazing\", \"in\", \"u\", \"it\", \"xd\", \"lol\", \"ive\", \"im\", \"could\", \"also\", \"many\",\n",
    "    \"im\", \"lol\", \"cant\", \"ing\"\n",
    "]\n",
    "\n",
    "# Function to preprocess and tokenize reviews\n",
    "def preprocess_reviews(df, text_column='review_text'):\n",
    "    \"\"\" Tokenizes, removes stopwords, and cleans review text. \"\"\"\n",
    "    # Drop missing values in the review_text column\n",
    "    df = df.dropna(subset=[text_column])\n",
    "\n",
    "    # Tokenize and clean reviews\n",
    "    docs1 = df[text_column].astype(str).apply(nltk.word_tokenize).tolist()\n",
    "\n",
    "    # Lowercasing, removing non-alphabetic tokens, and stopword removal\n",
    "    docs2 = [[w.lower() for w in doc] for doc in docs1]\n",
    "    docs3 = [[w for w in doc if re.search('^[a-z]+$', w)] for doc in docs2]\n",
    "    docs4 = [[w for w in doc if w not in stop_list] for doc in docs3]\n",
    "\n",
    "    return docs4\n",
    "\n",
    "# Convert DataFrame review column into processed documents\n",
    "docs = preprocess_reviews(df_filtered, text_column='review_text')\n",
    "\n",
    "# Create Dictionary\n",
    "dictionary = corpora.Dictionary(docs)\n",
    "\n",
    "# Convert documents into Bag of Words representation\n",
    "corpus_bow = [dictionary.doc2bow(doc) for doc in docs]\n",
    "\n",
    "# Train LDA Model\n",
    "num_topics = 10  # Adjust as needed\n",
    "lda_model = LdaModel(corpus=corpus_bow, id2word=dictionary, num_topics=num_topics, passes=10, random_state=42)\n",
    "\n",
    "# Print topics\n",
    "for idx, topic in lda_model.print_topics():\n",
    "    print(f\"Topic {idx}: {topic}\")\n",
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model = CoherenceModel(model=lda_model, texts=docs, dictionary=dictionary, coherence='c_v')\n",
    "coherence_score_LDA = coherence_model.get_coherence()\n",
    "print(f\"Coherence Score: {coherence_score_LDA}\")\n",
    "\n",
    "# Visualize LDA Topics\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim.prepare(lda_model, corpus_bow, dictionary)\n",
    "vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim.models import CoherenceModel\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# Remove rows with NaN values in the 'review_text' column\n",
    "df_filtered = df_filtered.dropna(subset=['review_text'])\n",
    "\n",
    "# Apply TF-IDF to the review text\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "X = vectorizer.fit_transform(df_filtered['review_text'])\n",
    "\n",
    "# Apply NMF with 10 topics\n",
    "nmf_model = NMF(n_components=10, random_state=42)\n",
    "W = nmf_model.fit_transform(X)\n",
    "H = nmf_model.components_\n",
    "\n",
    "# Extract top words for each topic from NMF\n",
    "def get_top_words_for_nmf(model, n_words=10):\n",
    "    \"\"\"Extract top n words for each topic in an NMF model.\"\"\"\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    top_words = []\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        top_words_idx = topic.argsort()[:-n_words-1:-1]\n",
    "        top_words.append([feature_names[i] for i in top_words_idx])\n",
    "    return top_words\n",
    "\n",
    "# Get top words for each NMF topic\n",
    "top_words_nmf = get_top_words_for_nmf(nmf_model)\n",
    "\n",
    "# Compute Coherence Score for NMF\n",
    "# For NMF, we will use Gensim's CoherenceModel, but need to create a custom list of top words\n",
    "# 'docs' is the preprocessed text from your dataset\n",
    "coherence_model_nmf = CoherenceModel(topics=top_words_nmf, texts=docs, dictionary=dictionary, coherence='c_v')\n",
    "coherence_score_nmf = coherence_model_nmf.get_coherence()\n",
    "print(f\"Coherence Score for NMF: {coherence_score_nmf}\")\n",
    "\n",
    "# Print topics for NMF\n",
    "for topic_idx, topic in enumerate(nmf_model.components_):\n",
    "    top_words_idx = topic.argsort()[:-11:-1]\n",
    "    top_words = [vectorizer.get_feature_names_out()[i] for i in top_words_idx]\n",
    "    print(f\"Topic {topic_idx}: {' '.join(top_words)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim.models import CoherenceModel\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# Apply TF-IDF to the review text\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "X = vectorizer.fit_transform(df_filtered['review_text'])\n",
    "\n",
    "# Apply LSA (Truncated SVD) with 10 topics\n",
    "lsa_model = TruncatedSVD(n_components=10, random_state=42)\n",
    "lsa_topics = lsa_model.fit_transform(X)\n",
    "\n",
    "# Extract top words for each topic from LSA\n",
    "def get_top_words_for_lsa(model, n_words=10):\n",
    "    \"\"\"Extract top n words for each topic in an LSA model.\"\"\"\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    top_words = []\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        top_words_idx = topic.argsort()[:-n_words-1:-1]\n",
    "        top_words.append([feature_names[i] for i in top_words_idx])\n",
    "    return top_words\n",
    "\n",
    "# Get top words for each LSA topic\n",
    "top_words_lsa = get_top_words_for_lsa(lsa_model)\n",
    "\n",
    "# Compute Coherence Score for LSA\n",
    "# For LSA, we will use Gensim's CoherenceModel, but need to create a custom list of top words\n",
    "# 'docs' is the preprocessed text from your dataset\n",
    "coherence_model_lsa = CoherenceModel(topics=top_words_lsa, texts=docs, dictionary=dictionary, coherence='c_v')\n",
    "coherence_score_lsa = coherence_model_lsa.get_coherence()\n",
    "print(f\"Coherence Score for LSA: {coherence_score_lsa}\")\n",
    "\n",
    "# Print topics for LSA\n",
    "for topic_idx, topic in enumerate(lsa_model.components_):\n",
    "    top_words_idx = topic.argsort()[:-11:-1]\n",
    "    top_words = [vectorizer.get_feature_names_out()[i] for i in top_words_idx]\n",
    "    print(f\"Topic {topic_idx}: {' '.join(top_words)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary libraries if not already installed\n",
    "# !pip install transformers\n",
    "# !pip install sentence-transformers\n",
    "# !pip install sklearn\n",
    "\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load BERT tokenizer and model from HuggingFace\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Alternatively, you can use a Sentence Transformer model for better document embeddings\n",
    "# model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Function to convert text to embeddings using BERT\n",
    "def get_bert_embeddings(texts, model, tokenizer):\n",
    "    embeddings = []\n",
    "    for text in texts:\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            # Take the mean of all token embeddings (pooling)\n",
    "            embedding = outputs.last_hidden_state.mean(dim=1).squeeze().cpu().numpy()\n",
    "            embeddings.append(embedding)\n",
    "    return np.array(embeddings)\n",
    "\n",
    "# Use the reviews from your DataFrame\n",
    "texts = df_filtered['review_text'].dropna().tolist()\n",
    "\n",
    "# Generate embeddings for all the reviews using BERT\n",
    "embeddings = get_bert_embeddings(texts, model, tokenizer)\n",
    "\n",
    "# Optional: Reduce dimensionality for easier visualization (e.g., PCA for 2D visualization)\n",
    "pca = PCA(n_components=2)\n",
    "reduced_embeddings = pca.fit_transform(embeddings)\n",
    "\n",
    "# Apply KMeans clustering to the BERT embeddings\n",
    "num_topics = 10  # Adjust the number of topics as necessary\n",
    "kmeans = KMeans(n_clusters=num_topics, random_state=42)\n",
    "kmeans.fit(embeddings)\n",
    "\n",
    "# Get the topic/cluster assignments for each document\n",
    "topic_assignments = kmeans.labels_\n",
    "\n",
    "# Optionally, calculate the silhouette score for the clustering quality\n",
    "silhouette_avg = silhouette_score(embeddings, topic_assignments)\n",
    "print(f'Silhouette Score: {silhouette_avg}')\n",
    "\n",
    "# Visualize the clusters (Optional: PCA reduction to 2D for visualization)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(reduced_embeddings[:, 0], reduced_embeddings[:, 1], c=topic_assignments, cmap='viridis')\n",
    "plt.colorbar()\n",
    "plt.title(\"BERT-based Clusters (PCA reduced to 2D)\")\n",
    "plt.show()\n",
    "\n",
    "# Optionally: Print the top words for each topic\n",
    "# To find the top words for each cluster, we can get the most frequent terms in each cluster using the BERT embeddings\n",
    "# However, BERT itself doesn't produce explicit words for topics, so we'll just visualize the clusters.\n",
    "\n",
    "# You can also create word clouds or extract top words per cluster by clustering the terms (this is more advanced).\n",
    "\n",
    "# To print out the topic assignments for each document:\n",
    "df_filtered['topic'] = topic_assignments\n",
    "print(df_filtered[['review_text', 'topic']].head())\n",
    "\n",
    "# Optional: If you want to use SentenceTransformers to generate better embeddings\n",
    "# model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "# embeddings = model.encode(texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "# Vectorize the text using TF-IDF\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "X = vectorizer.fit_transform(df_filtered['review_text'])\n",
    "\n",
    "# Apply LSA (SVD)\n",
    "lsa_model = TruncatedSVD(n_components=10, random_state=42)\n",
    "lsa_topic_matrix = lsa_model.fit_transform(X)\n",
    "\n",
    "# Print topics\n",
    "terms = vectorizer.get_feature_names_out()\n",
    "for topic_idx, topic in enumerate(lsa_model.components_):\n",
    "    print(f\"Topic {topic_idx}:\")\n",
    "    print([terms[i] for i in topic.argsort()[:-11:-1]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualise coherence score based on number of topics\n",
    "import matplotlib.pyplot as plt\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# Function to compute coherence score for different topic numbers\n",
    "def compute_coherence_values(dictionary, corpus, texts, start=2, limit=15, step=1):\n",
    "    coherence_values = []\n",
    "    topic_range = range(start, limit, step)\n",
    "\n",
    "    for num_topics in topic_range:\n",
    "        # Train LDA model with num_topics\n",
    "        lda_model = gensim.models.LdaModel(corpus=corpus, id2word=dictionary, num_topics=num_topics, passes=10, random_state=42)\n",
    "\n",
    "        # Compute coherence score\n",
    "        coherence_model = CoherenceModel(model=lda_model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "        coherence_values.append(coherence_model.get_coherence())\n",
    "\n",
    "    return topic_range, coherence_values\n",
    "\n",
    "# Define topic range and compute coherence scores\n",
    "topic_range, coherence_values = compute_coherence_values(dictionary, corpus_bow, docs, start=2, limit=15, step=1)\n",
    "\n",
    "# Plot Coherence Score graph\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(topic_range, coherence_values, marker='o', linestyle='-')\n",
    "plt.xlabel(\"Number of Topics\")\n",
    "plt.ylabel(\"Coherence Score\")\n",
    "plt.title(\"Coherence Score vs. Number of Topics\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
