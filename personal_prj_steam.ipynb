{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. LIBRARIES USED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "import re\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "import spacy\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. DATA SELECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_df = pd.read_csv('dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_games = [\n",
    "    #RPG\n",
    "    \"Dota 2\",\n",
    "    \"The Elder Scrolls V: Skyrim\",\n",
    "    \"The Witcher 3: Wild Hunt\",\n",
    "\n",
    "    #FPS\n",
    "    \"Call of Duty: Modern Warfare 3\",\n",
    "    \"Counter-Strike\",\n",
    "    \"DOOM\",\n",
    "\n",
    "    #Sports\n",
    "    \"NBA 2K16\",\n",
    "    \"Rocket League\",\n",
    "    \"Football Manager 2016\"\n",
    "    ]\n",
    "\n",
    "df_filtered_games = original_df[original_df['app_name'].isin(chosen_games)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>app_id</th>\n",
       "      <th>app_name</th>\n",
       "      <th>review_text</th>\n",
       "      <th>review_score</th>\n",
       "      <th>review_votes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>Counter-Strike</td>\n",
       "      <td>Ruined my life.</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>Counter-Strike</td>\n",
       "      <td>This will be more of a ''my experience with th...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>Counter-Strike</td>\n",
       "      <td>This game saved my virginity.</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>Counter-Strike</td>\n",
       "      <td>• Do you like original games? • Do you like ga...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td>Counter-Strike</td>\n",
       "      <td>Easy to learn, hard to master.</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   app_id        app_name                                        review_text  \\\n",
       "0      10  Counter-Strike                                    Ruined my life.   \n",
       "1      10  Counter-Strike  This will be more of a ''my experience with th...   \n",
       "2      10  Counter-Strike                      This game saved my virginity.   \n",
       "3      10  Counter-Strike  • Do you like original games? • Do you like ga...   \n",
       "4      10  Counter-Strike           Easy to learn, hard to master.             \n",
       "\n",
       "   review_score  review_votes  \n",
       "0             1             0  \n",
       "1             1             1  \n",
       "2             1             0  \n",
       "3             1             0  \n",
       "4             1             1  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_filtered_games.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. DATA PRE-PROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import string\n",
    "# from nltk.corpus import stopwords, wordnet\n",
    "# from nltk.tokenize import word_tokenize\n",
    "# from nltk.stem import WordNetLemmatizer\n",
    "# from nltk import pos_tag\n",
    "# import nltk\n",
    "\n",
    "# # Download required resources\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# lemmatizer = WordNetLemmatizer()\n",
    "# stop_words = set(stopwords.words('english'))  # Load stopwords once\n",
    "\n",
    "# # Function to map POS tags to WordNet format\n",
    "# def get_wordnet_pos(tag):\n",
    "#     if tag.startswith('J'):\n",
    "#         return wordnet.ADJ\n",
    "#     elif tag.startswith('V'):\n",
    "#         return wordnet.VERB\n",
    "#     elif tag.startswith('N'):\n",
    "#         return wordnet.NOUN\n",
    "#     elif tag.startswith('R'):\n",
    "#         return wordnet.ADV\n",
    "#     else:\n",
    "#         return wordnet.NOUN  # Default to noun\n",
    "\n",
    "# # Define a function to preprocess text\n",
    "# def preprocess_text(text):\n",
    "#     text = text.lower()  # Convert to lowercase\n",
    "#     text = text.translate(str.maketrans('', '', string.punctuation))  # Remove punctuation\n",
    "#     tokens = word_tokenize(text)  # Tokenize text\n",
    "#     tokens = [word for word in tokens if word not in stop_words]  # Remove stopwords\n",
    "    \n",
    "#     # Get POS tags and lemmatize accordingly\n",
    "#     pos_tags = pos_tag(tokens)\n",
    "#     tokens = [lemmatizer.lemmatize(word, get_wordnet_pos(tag)) for word, tag in pos_tags]\n",
    "\n",
    "#     return ' '.join(tokens)\n",
    "\n",
    "# # Apply the function to the review_text column\n",
    "# df['cleaned_review_text'] = df['review_text'].apply(lambda x: preprocess_text(x) if isinstance(x, str) else x)\n",
    "\n",
    "# print(df[['review_text', 'cleaned_review_text']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" import string\\nfrom nltk.corpus import stopwords\\nfrom nltk.tokenize import word_tokenize\\nimport nltk\\n\\n# Download stopwords if not already downloaded\\nnltk.download('stopwords')\\nnltk.download('punkt')\\n\\n# Define a function to preprocess text\\ndef preprocess_text(text):\\n    # Convert to lowercase\\n    text = text.lower()\\n    # Remove punctuation\\n    text = text.translate(str.maketrans('', '', string.punctuation))\\n    # Tokenize text\\n    tokens = word_tokenize(text)\\n    # Remove stopwords\\n    tokens = [word for word in tokens if word not in stopwords.words('english')]\\n    return ' '.join(tokens)\\n\\n# Apply the function to the review_text column\\ndf['cleaned_review_text'] = df['review_text'].apply(lambda x: preprocess_text(x) if isinstance(x, str) else x)\\n\\nprint(df[['review_text', 'cleaned_review_text']].head()) \""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" import string\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "\n",
    "# Download stopwords if not already downloaded\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Define a function to preprocess text\n",
    "def preprocess_text(text):\n",
    "    text = text.lower() # Convert to lowercase\n",
    "    \n",
    "    text = text.translate(str.maketrans('', '', string.punctuation)) # Remove punctuation\n",
    "    \n",
    "    tokens = word_tokenize(text)# Tokenize text\n",
    "    \n",
    "    tokens = [word for word in tokens if word not in stopwords.words('english')] # Remove stopwords\n",
    "\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Apply the function to the review_text column\n",
    "df['cleaned_review_text'] = df['review_text'].apply(lambda x: preprocess_text(x) if isinstance(x, str) else x)\n",
    "\n",
    "print(df[['review_text', 'cleaned_review_text']].head()) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 cleaned_review_text  review_score  sentiment  \\\n",
      "0                                        ruined life             1    -0.4767   \n",
      "1  experience game type review saying things like...             1     0.9961   \n",
      "2                               game saved virginity             1     0.4215   \n",
      "3  like original games like games dont lag like g...             1     0.8817   \n",
      "4                             easy learn hard master             1     0.3612   \n",
      "\n",
      "   sarcasm  \n",
      "0        1  \n",
      "1        0  \n",
      "2        0  \n",
      "3        0  \n",
      "4        0  \n"
     ]
    }
   ],
   "source": [
    "# Define a function to detect sarcasm\n",
    "def detect_sarcasm(row):\n",
    "    # Check for rating-text mismatch\n",
    "    if row['review_score'] == 1 and row['sentiment'] < 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# Apply the function to the DataFrame\n",
    "df['sarcasm'] = df.apply(detect_sarcasm, axis=1)\n",
    "\n",
    "print(df[['cleaned_review_text', 'review_score', 'sentiment', 'sarcasm']].head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sarcasm\n",
      "0    182283\n",
      "1     22106\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df['sarcasm'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                                          ruined life\n",
      "1    experience game type review saying things like...\n",
      "2                                 game saved virginity\n",
      "3    like original games like games dont lag like g...\n",
      "4                               easy learn hard master\n",
      "5                                      r revolver play\n",
      "6                        still better call duty ghosts\n",
      "7    cant buy skins cases keys stickers gaben cant ...\n",
      "8    counterstrike ok years unlimited fun friends f...\n",
      "9    every server spanish french fluently swear lan...\n",
      "Name: cleaned_review_text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df['cleaned_review_text'].head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "review_votes\n",
      "0    173422\n",
      "1     30967\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df['review_votes'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   aa  aaa  aaaa  aaaaa  aaaaaaaaaa  aaaaaaaaaaa  \\\n",
      "0   0    0     0      0           0            0   \n",
      "1   0    0     0      0           0            0   \n",
      "2   0    0     0      0           0            0   \n",
      "3   0    0     0      0           0            0   \n",
      "4   0    0     0      0           0            0   \n",
      "\n",
      "   aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa  \\\n",
      "0                                 0   \n",
      "1                                 0   \n",
      "2                                 0   \n",
      "3                                 0   \n",
      "4                                 0   \n",
      "\n",
      "   aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa  \\\n",
      "0                                                  0                \n",
      "1                                                  0                \n",
      "2                                                  0                \n",
      "3                                                  0                \n",
      "4                                                  0                \n",
      "\n",
      "   aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaahhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh  \\\n",
      "0                                                  0                            \n",
      "1                                                  0                            \n",
      "2                                                  0                            \n",
      "3                                                  0                            \n",
      "4                                                  0                            \n",
      "\n",
      "   aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaahhhh  ...  zynga  zyzz  zz  zzz  zzzz  \\\n",
      "0                                        0  ...      0     0   0    0     0   \n",
      "1                                        0  ...      0     0   0    0     0   \n",
      "2                                        0  ...      0     0   0    0     0   \n",
      "3                                        0  ...      0     0   0    0     0   \n",
      "4                                        0  ...      0     0   0    0     0   \n",
      "\n",
      "   zzzzquiet  zzzzz  zzzzzz  zzzzzzzzzzzz  zzzzzzzzzzzzzzzzzzz  \n",
      "0          0      0       0             0                    0  \n",
      "1          0      0       0             0                    0  \n",
      "2          0      0       0             0                    0  \n",
      "3          0      0       0             0                    0  \n",
      "4          0      0       0             0                    0  \n",
      "\n",
      "[5 rows x 83780 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "import scipy.sparse\n",
    "\n",
    "# Initialize the TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "df = df[df[\"sarcasm\"] == 0]\n",
    "# Fit and transform the cleaned_review_text column\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(df['cleaned_review_text'].dropna())\n",
    "\n",
    "# Convert the TF-IDF matrix to a DataFrame (using sparse matrix format)\n",
    "tfidf_df = pd.DataFrame.sparse.from_spmatrix(tfidf_matrix, columns=tfidf_vectorizer.get_feature_names_out())\n",
    "\n",
    "print(tfidf_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-zero entries: 3243575\n",
      "Total elements: 14775189460\n"
     ]
    }
   ],
   "source": [
    "nonzero_count = tfidf_matrix.nnz\n",
    "total_elements = tfidf_matrix.shape[0] * tfidf_matrix.shape[1]\n",
    "print(\"Non-zero entries:\", nonzero_count)\n",
    "print(\"Total elements:\", total_elements)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "account     0.065737\n",
      "advanced    0.069125\n",
      "ago         0.058497\n",
      "already     0.095798\n",
      "also        0.038517\n",
      "              ...   \n",
      "way          0.12028\n",
      "weeks       0.064289\n",
      "wish        0.053167\n",
      "wouldnt      0.11771\n",
      "years       0.044134\n",
      "Name: 0, Length: 140, dtype: Sparse[float64, 0]\n"
     ]
    }
   ],
   "source": [
    "# Print the nonzero tokens for the first review\n",
    "nonzero_tokens = tfidf_df.iloc[0][tfidf_df.iloc[0] != 0]\n",
    "print(nonzero_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nonzero counts for each row: [140   3  13 ...   3 124  22]\n"
     ]
    }
   ],
   "source": [
    "nonzero_per_row = tfidf_matrix.getnnz(axis=1)\n",
    "print(\"Nonzero counts for each row:\", nonzero_per_row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_8416\\185708010.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_clean.drop(columns=['review_text', 'review_votes'], inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# --- Data Preparation ---\n",
    "# Filter the DataFrame to include only rows with a cleaned review text.\n",
    "# This ensures that the TF-IDF features align with your target variable.\n",
    "df_clean = df.dropna(subset=['cleaned_review_text'])\n",
    "df_clean.drop(columns=['review_text', 'review_votes'], inplace=True)\n",
    "df_clean = df_clean[df_clean[\"sarcasm\"] == 0]\n",
    "\n",
    "# IMPORTANT:\n",
    "# The tfidf_matrix should have been computed on df['cleaned_review_text'].dropna()\n",
    "# so its rows correspond to df_clean.\n",
    "# If that’s not the case, re-run the vectorization on df_clean, e.g.:\n",
    "# tfidf_matrix = tfidf_vectorizer.fit_transform(df_clean['cleaned_review_text'])\n",
    "\n",
    "# Define features and target\n",
    "X = tfidf_matrix          # TF-IDF features (sparse matrix)\n",
    "y = df_clean['review_score']  # Target variable: 1 (positive/recommend) or -1 (negative/not recommend)\n",
    "\n",
    "# --- Train-Test Split ---\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# --- Oversampling using SMOTE ---\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Initialize SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "\n",
    "# Apply SMOTE to the training data\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8759355863007484\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.49      0.87      0.62      4161\n",
      "           1       0.98      0.88      0.93     31111\n",
      "\n",
      "    accuracy                           0.88     35272\n",
      "   macro avg       0.73      0.87      0.77     35272\n",
      "weighted avg       0.92      0.88      0.89     35272\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 3601   560]\n",
      " [ 3816 27295]]\n"
     ]
    }
   ],
   "source": [
    "# --- Model Training ---\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Initialize and train the Logistic Regression classifier\n",
    "lr = LogisticRegression(max_iter=1000, class_weight='balanced')\n",
    "lr.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# --- Model Evaluation ---\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = lr.predict(X_test)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of tfidf_matrix: (198459, 93228)\n",
      "Number of rows in df_clean: 198459\n",
      "tfidf_matrix was computed on df_clean['cleaned_review_text']\n"
     ]
    }
   ],
   "source": [
    "# Check the shape of tfidf_matrix\n",
    "print(\"Shape of tfidf_matrix:\", tfidf_matrix.shape)\n",
    "\n",
    "# Check the number of rows in df_clean\n",
    "print(\"Number of rows in df_clean:\", df_clean.shape[0])\n",
    "\n",
    "# Verify if they match\n",
    "if tfidf_matrix.shape[0] == df_clean.shape[0]:\n",
    "    print(\"tfidf_matrix was computed on df_clean['cleaned_review_text']\")\n",
    "else:\n",
    "    print(\"tfidf_matrix was NOT computed on df_clean['cleaned_review_text']\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.853774060264033\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.40      0.83      0.54      4161\n",
      "           1       0.98      0.86      0.91     35531\n",
      "\n",
      "    accuracy                           0.85     39692\n",
      "   macro avg       0.69      0.84      0.73     39692\n",
      "weighted avg       0.92      0.85      0.87     39692\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 3438   723]\n",
      " [ 5081 30450]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# Initialize and train the SVM classifier\n",
    "svm = SVC(kernel='linear', class_weight='balanced', random_state=42)\n",
    "svm.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred_svm = svm.predict(X_test)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_svm))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_svm))\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_svm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['svm_model.pkl']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Export the SVM model to a file\n",
    "joblib.dump(svm, 'svm_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9312707850448453\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.67      0.68      0.67      4161\n",
      "           1       0.96      0.96      0.96     35531\n",
      "\n",
      "    accuracy                           0.93     39692\n",
      "   macro avg       0.82      0.82      0.82     39692\n",
      "weighted avg       0.93      0.93      0.93     39692\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 2810  1351]\n",
      " [ 1377 34154]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Initialize and train the Random Forest classifier\n",
    "rf = RandomForestClassifier(n_estimators=100, class_weight='balanced', random_state=42)\n",
    "rf.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_rf))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_rf))\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7565504383754913\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.27      0.77      0.40      4161\n",
      "           1       0.97      0.75      0.85     35531\n",
      "\n",
      "    accuracy                           0.76     39692\n",
      "   macro avg       0.62      0.76      0.62     39692\n",
      "weighted avg       0.89      0.76      0.80     39692\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 3220   941]\n",
      " [ 8722 26809]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# Initialize and train the Gradient Boosting classifier\n",
    "gb = GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
    "gb.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred_gb = gb.predict(X_test)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_gb))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_gb))\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_gb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gradient_boosting_model.pkl']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Export the Random Forest model to a file\n",
    "joblib.dump(rf, 'random_forest_model.pkl')\n",
    "\n",
    "# Export the Gradient Boosting model to a file\n",
    "joblib.dump(gb, 'gradient_boosting_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>app_id</th>\n",
       "      <th>app_name</th>\n",
       "      <th>review_score</th>\n",
       "      <th>tokens</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>cleaned_review_text</th>\n",
       "      <th>sarcasm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>Counter-Strike</td>\n",
       "      <td>1</td>\n",
       "      <td>['experience', 'game', 'type', 'review', 'sayi...</td>\n",
       "      <td>0.9961</td>\n",
       "      <td>experience game type review saying things like...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>Counter-Strike</td>\n",
       "      <td>1</td>\n",
       "      <td>['game', 'saved', 'virginity']</td>\n",
       "      <td>0.4215</td>\n",
       "      <td>game saved virginity</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>Counter-Strike</td>\n",
       "      <td>1</td>\n",
       "      <td>['like', 'original', 'games', 'like', 'games',...</td>\n",
       "      <td>0.8817</td>\n",
       "      <td>like original games like games dont lag like g...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td>Counter-Strike</td>\n",
       "      <td>1</td>\n",
       "      <td>['easy', 'learn', 'hard', 'master']</td>\n",
       "      <td>0.3612</td>\n",
       "      <td>easy learn hard master</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10</td>\n",
       "      <td>Counter-Strike</td>\n",
       "      <td>1</td>\n",
       "      <td>['r', 'revolver', 'play']</td>\n",
       "      <td>0.0516</td>\n",
       "      <td>r revolver play</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   app_id        app_name  review_score  \\\n",
       "1      10  Counter-Strike             1   \n",
       "2      10  Counter-Strike             1   \n",
       "3      10  Counter-Strike             1   \n",
       "4      10  Counter-Strike             1   \n",
       "5      10  Counter-Strike             1   \n",
       "\n",
       "                                              tokens  sentiment  \\\n",
       "1  ['experience', 'game', 'type', 'review', 'sayi...     0.9961   \n",
       "2                     ['game', 'saved', 'virginity']     0.4215   \n",
       "3  ['like', 'original', 'games', 'like', 'games',...     0.8817   \n",
       "4                ['easy', 'learn', 'hard', 'master']     0.3612   \n",
       "5                          ['r', 'revolver', 'play']     0.0516   \n",
       "\n",
       "                                 cleaned_review_text  sarcasm  \n",
       "1  experience game type review saying things like...        0  \n",
       "2                               game saved virginity        0  \n",
       "3  like original games like games dont lag like g...        0  \n",
       "4                             easy learn hard master        0  \n",
       "5                                    r revolver play        0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "\n",
    "# Load BERT tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
    "\n",
    "# Tokenization function\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"cleaned_review_text\"], padding=\"max_length\", truncation=True, max_length=128)\n",
    "\n",
    "df_clean['review_score'] = df_clean['review_score'].apply(lambda x: 1 if x == 1 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())  # Should return True if GPU is available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 176357/176357 [01:09<00:00, 2543.02 examples/s]\n",
      "c:\\Python312\\Lib\\site-packages\\transformers\\training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_8416\\4280998099.py:26: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "c:\\Python312\\Lib\\site-packages\\accelerate\\accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='52908' max='52908' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [52908/52908 1:10:39, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.368000</td>\n",
       "      <td>0.369587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.306400</td>\n",
       "      <td>0.325510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.241900</td>\n",
       "      <td>0.266819</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5326' max='4409' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4409/4409 10:42]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results: {'eval_loss': 0.2668187618255615, 'eval_runtime': 58.7291, 'eval_samples_per_second': 600.589, 'eval_steps_per_second': 75.074, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "# Convert to Hugging Face dataset\n",
    "dataset = Dataset.from_pandas(df_clean)\n",
    "dataset = dataset.map(tokenize_function, batched=True)\n",
    "dataset = dataset.remove_columns([\"tokens\"])\n",
    "dataset = dataset.rename_column(\"review_score\", \"labels\")\n",
    "dataset.set_format(\"torch\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "# Split dataset\n",
    "train_test_split = dataset.train_test_split(test_size=0.2)\n",
    "train_dataset = train_test_split[\"train\"]\n",
    "test_dataset = train_test_split[\"test\"]\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    fp16=True if torch.cuda.is_available() else False\n",
    ")\n",
    "\n",
    "# Define trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# Train model\n",
    "trainer.train()\n",
    "\n",
    "eval_results = trainer.evaluate()\n",
    "print(f\"Evaluation results: {eval_results}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mcompute_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43meval_results\u001b[49m\u001b[43m)\u001b[49m)\n",
      "Cell \u001b[1;32mIn[10], line 39\u001b[0m, in \u001b[0;36mcompute_metrics\u001b[1;34m(eval_pred)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_metrics\u001b[39m(eval_pred):\n\u001b[1;32m---> 39\u001b[0m     logits, labels \u001b[38;5;241m=\u001b[39m eval_pred\n\u001b[0;32m     40\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(logits, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m: accuracy_score(labels, predictions)}\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Save model and tokenizer\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39msave_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./saved_model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39msave_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./saved_model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel saved successfully.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# Save model and tokenizer\n",
    "model.save_pretrained(\"./saved_model\")\n",
    "tokenizer.save_pretrained(\"./saved_model\")\n",
    "print(\"Model saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.0+cpu\n",
      "0.34.2\n",
      "4.49.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import accelerate\n",
    "import transformers\n",
    "print(torch.__version__)\n",
    "print(accelerate.__version__)\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
