{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "app_name\n",
      "Dota 2                            73541\n",
      "Rocket League                     54227\n",
      "The Witcher 3: Wild Hunt          31853\n",
      "DOOM                              20682\n",
      "Counter-Strike                    12353\n",
      "Call of Duty: Modern Warfare 3     5197\n",
      "NBA 2K16                           2534\n",
      "The Elder Scrolls V: Skyrim        2260\n",
      "Football Manager 2016              1742\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('preprocessed_reviews.csv')\n",
    "\n",
    "# Get the number of reviews for each product\n",
    "product_review_count = df['app_name'].value_counts()\n",
    "\n",
    "\n",
    "print(product_review_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" import string\\nfrom nltk.corpus import stopwords\\nfrom nltk.tokenize import word_tokenize\\nimport nltk\\n\\n# Download stopwords if not already downloaded\\nnltk.download('stopwords')\\nnltk.download('punkt')\\n\\n# Define a function to preprocess text\\ndef preprocess_text(text):\\n    # Convert to lowercase\\n    text = text.lower()\\n    # Remove punctuation\\n    text = text.translate(str.maketrans('', '', string.punctuation))\\n    # Tokenize text\\n    tokens = word_tokenize(text)\\n    # Remove stopwords\\n    tokens = [word for word in tokens if word not in stopwords.words('english')]\\n    return ' '.join(tokens)\\n\\n# Apply the function to the review_text column\\ndf['cleaned_review_text'] = df['review_text'].apply(lambda x: preprocess_text(x) if isinstance(x, str) else x)\\n\\nprint(df[['review_text', 'cleaned_review_text']].head()) \""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "\n",
    "# Download stopwords if not already downloaded\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Define a function to preprocess text\n",
    "def preprocess_text(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    # Tokenize text\n",
    "    tokens = word_tokenize(text)\n",
    "    # Remove stopwords\n",
    "    tokens = [word for word in tokens if word not in stopwords.words('english')]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Apply the function to the review_text column\n",
    "df['cleaned_review_text'] = df['review_text'].apply(lambda x: preprocess_text(x) if isinstance(x, str) else x)\n",
    "\n",
    "print(df[['review_text', 'cleaned_review_text']].head()) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 cleaned_review_text  review_score  sentiment  \\\n",
      "0                                        ruined life             1    -0.4767   \n",
      "1  experience game type review saying things like...             1     0.9961   \n",
      "2                               game saved virginity             1     0.4215   \n",
      "3  like original games like games dont lag like g...             1     0.8817   \n",
      "4                             easy learn hard master             1     0.3612   \n",
      "\n",
      "   sarcasm  \n",
      "0        1  \n",
      "1        0  \n",
      "2        0  \n",
      "3        0  \n",
      "4        0  \n"
     ]
    }
   ],
   "source": [
    "# Define a function to detect sarcasm\n",
    "def detect_sarcasm(row):\n",
    "    # Check for rating-text mismatch\n",
    "    if row['review_score'] == 1 and row['sentiment'] < 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# Apply the function to the DataFrame\n",
    "df['sarcasm'] = df.apply(detect_sarcasm, axis=1)\n",
    "\n",
    "print(df[['cleaned_review_text', 'review_score', 'sentiment', 'sarcasm']].head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sarcasm\n",
      "0    182283\n",
      "1     22106\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df['sarcasm'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                                          ruined life\n",
      "1    experience game type review saying things like...\n",
      "2                                 game saved virginity\n",
      "3    like original games like games dont lag like g...\n",
      "4                               easy learn hard master\n",
      "5                                      r revolver play\n",
      "6                        still better call duty ghosts\n",
      "7    cant buy skins cases keys stickers gaben cant ...\n",
      "8    counterstrike ok years unlimited fun friends f...\n",
      "9    every server spanish french fluently swear lan...\n",
      "Name: cleaned_review_text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df['cleaned_review_text'].head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "review_votes\n",
      "0    173422\n",
      "1     30967\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df['review_votes'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   aa  aaa  aaaa  aaaaa  aaaaaaaaaa  aaaaaaaaaaa  \\\n",
      "0   0    0     0      0           0            0   \n",
      "1   0    0     0      0           0            0   \n",
      "2   0    0     0      0           0            0   \n",
      "3   0    0     0      0           0            0   \n",
      "4   0    0     0      0           0            0   \n",
      "\n",
      "   aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa  \\\n",
      "0                                 0   \n",
      "1                                 0   \n",
      "2                                 0   \n",
      "3                                 0   \n",
      "4                                 0   \n",
      "\n",
      "   aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa  \\\n",
      "0                                                  0                \n",
      "1                                                  0                \n",
      "2                                                  0                \n",
      "3                                                  0                \n",
      "4                                                  0                \n",
      "\n",
      "   aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaahhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh  \\\n",
      "0                                                  0                            \n",
      "1                                                  0                            \n",
      "2                                                  0                            \n",
      "3                                                  0                            \n",
      "4                                                  0                            \n",
      "\n",
      "   aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaahhhh  ...  zyzz  zz  zzz  zzzz  \\\n",
      "0                                        0  ...     0   0    0     0   \n",
      "1                                        0  ...     0   0    0     0   \n",
      "2                                        0  ...     0   0    0     0   \n",
      "3                                        0  ...     0   0    0     0   \n",
      "4                                        0  ...     0   0    0     0   \n",
      "\n",
      "   zzzzquiet  zzzzz  zzzzzz  zzzzzzzzzzzz  zzzzzzzzzzzzzzzzzzz  \\\n",
      "0          0      0       0             0                    0   \n",
      "1          0      0       0             0                    0   \n",
      "2          0      0       0             0                    0   \n",
      "3          0      0       0             0                    0   \n",
      "4          0      0       0             0                    0   \n",
      "\n",
      "   zzzzzzzzzzzzzzzzzzzzzzzzzzz  \n",
      "0                            0  \n",
      "1                            0  \n",
      "2                            0  \n",
      "3                            0  \n",
      "4                            0  \n",
      "\n",
      "[5 rows x 93228 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "import scipy.sparse\n",
    "\n",
    "# Initialize the TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the cleaned_review_text column\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(df['cleaned_review_text'].dropna())\n",
    "\n",
    "# Convert the TF-IDF matrix to a DataFrame (using sparse matrix format)\n",
    "tfidf_df = pd.DataFrame.sparse.from_spmatrix(tfidf_matrix, columns=tfidf_vectorizer.get_feature_names_out())\n",
    "\n",
    "print(tfidf_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-zero entries: 3714730\n",
      "Total elements: 18501935652\n"
     ]
    }
   ],
   "source": [
    "nonzero_count = tfidf_matrix.nnz\n",
    "total_elements = tfidf_matrix.shape[0] * tfidf_matrix.shape[1]\n",
    "print(\"Non-zero entries:\", nonzero_count)\n",
    "print(\"Total elements:\", total_elements)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "life      0.570045\n",
      "ruined    0.821614\n",
      "Name: 0, dtype: Sparse[float64, 0]\n"
     ]
    }
   ],
   "source": [
    "# Print the nonzero tokens for the first review\n",
    "nonzero_tokens = tfidf_df.iloc[0][tfidf_df.iloc[0] != 0]\n",
    "print(nonzero_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nonzero counts for each row: [  2 140   3 ...   3 124  22]\n"
     ]
    }
   ],
   "source": [
    "nonzero_per_row = tfidf_matrix.getnnz(axis=1)\n",
    "print(\"Nonzero counts for each row:\", nonzero_per_row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Data Preparation ---\n",
    "# Filter the DataFrame to include only rows with a cleaned review text.\n",
    "# This ensures that the TF-IDF features align with your target variable.\n",
    "df_clean = df.dropna(subset=['cleaned_review_text'])\n",
    "\n",
    "# IMPORTANT:\n",
    "# The tfidf_matrix should have been computed on df['cleaned_review_text'].dropna()\n",
    "# so its rows correspond to df_clean.\n",
    "# If that’s not the case, re-run the vectorization on df_clean, e.g.:\n",
    "# tfidf_matrix = tfidf_vectorizer.fit_transform(df_clean['cleaned_review_text'])\n",
    "\n",
    "# Define features and target\n",
    "X = tfidf_matrix          # TF-IDF features (sparse matrix)\n",
    "y = df_clean['review_score']  # Target variable: 1 (positive/recommend) or -1 (negative/not recommend)\n",
    "\n",
    "# --- Train-Test Split ---\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# --- Oversampling using SMOTE ---\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Initialize SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "\n",
    "# Apply SMOTE to the training data\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8552353119016427\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.41      0.84      0.55      4161\n",
      "           1       0.98      0.86      0.91     35531\n",
      "\n",
      "    accuracy                           0.86     39692\n",
      "   macro avg       0.69      0.85      0.73     39692\n",
      "weighted avg       0.92      0.86      0.88     39692\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 3503   658]\n",
      " [ 5088 30443]]\n"
     ]
    }
   ],
   "source": [
    "# --- Model Training ---\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Initialize and train the Logistic Regression classifier\n",
    "lr = LogisticRegression(max_iter=1000, class_weight='balanced')\n",
    "lr.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# --- Model Evaluation ---\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = lr.predict(X_test)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of tfidf_matrix: (198459, 93228)\n",
      "Number of rows in df_clean: 198459\n",
      "tfidf_matrix was computed on df_clean['cleaned_review_text']\n"
     ]
    }
   ],
   "source": [
    "# Check the shape of tfidf_matrix\n",
    "print(\"Shape of tfidf_matrix:\", tfidf_matrix.shape)\n",
    "\n",
    "# Check the number of rows in df_clean\n",
    "print(\"Number of rows in df_clean:\", df_clean.shape[0])\n",
    "\n",
    "# Verify if they match\n",
    "if tfidf_matrix.shape[0] == df_clean.shape[0]:\n",
    "    print(\"tfidf_matrix was computed on df_clean['cleaned_review_text']\")\n",
    "else:\n",
    "    print(\"tfidf_matrix was NOT computed on df_clean['cleaned_review_text']\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.853774060264033\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.40      0.83      0.54      4161\n",
      "           1       0.98      0.86      0.91     35531\n",
      "\n",
      "    accuracy                           0.85     39692\n",
      "   macro avg       0.69      0.84      0.73     39692\n",
      "weighted avg       0.92      0.85      0.87     39692\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 3438   723]\n",
      " [ 5081 30450]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# Initialize and train the SVM classifier\n",
    "svm = SVC(kernel='linear', class_weight='balanced', random_state=42)\n",
    "svm.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred_svm = svm.predict(X_test)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_svm))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_svm))\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_svm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['svm_model.pkl']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Export the SVM model to a file\n",
    "joblib.dump(svm, 'svm_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9312707850448453\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.67      0.68      0.67      4161\n",
      "           1       0.96      0.96      0.96     35531\n",
      "\n",
      "    accuracy                           0.93     39692\n",
      "   macro avg       0.82      0.82      0.82     39692\n",
      "weighted avg       0.93      0.93      0.93     39692\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 2810  1351]\n",
      " [ 1377 34154]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Initialize and train the Random Forest classifier\n",
    "rf = RandomForestClassifier(n_estimators=100, class_weight='balanced', random_state=42)\n",
    "rf.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_rf))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_rf))\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7565504383754913\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.27      0.77      0.40      4161\n",
      "           1       0.97      0.75      0.85     35531\n",
      "\n",
      "    accuracy                           0.76     39692\n",
      "   macro avg       0.62      0.76      0.62     39692\n",
      "weighted avg       0.89      0.76      0.80     39692\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 3220   941]\n",
      " [ 8722 26809]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# Initialize and train the Gradient Boosting classifier\n",
    "gb = GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
    "gb.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred_gb = gb.predict(X_test)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_gb))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_gb))\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_gb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gradient_boosting_model.pkl']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Export the Random Forest model to a file\n",
    "joblib.dump(rf, 'random_forest_model.pkl')\n",
    "\n",
    "# Export the Gradient Boosting model to a file\n",
    "joblib.dump(gb, 'gradient_boosting_model.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
